{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":461237,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":373360,"modelId":394206}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-06T19:39:52.700272Z","iopub.execute_input":"2025-07-06T19:39:52.700997Z","iopub.status.idle":"2025-07-06T19:39:52.972622Z","shell.execute_reply.started":"2025-07-06T19:39:52.700965Z","shell.execute_reply":"2025-07-06T19:39:52.971923Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/resume/other/default/1/test.pdf\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install dotenv \"unstructured[pdf]\" langchain-community transformers langchain_huggingface faiss-gpu-cu12","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T19:39:56.540142Z","iopub.execute_input":"2025-07-06T19:39:56.540488Z","iopub.status.idle":"2025-07-06T19:41:33.510055Z","shell.execute_reply.started":"2025-07-06T19:39:56.540467Z","shell.execute_reply":"2025-07-06T19:41:33.509111Z"}},"outputs":[{"name":"stdout","text":"Collecting dotenv\n  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\nCollecting langchain-community\n  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nCollecting langchain_huggingface\n  Downloading langchain_huggingface-0.3.0-py3-none-any.whl.metadata (996 bytes)\nCollecting faiss-gpu-cu12\n  Downloading faiss_gpu_cu12-1.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting unstructured[pdf]\n  Downloading unstructured-0.18.3-py3-none-any.whl.metadata (24 kB)\nCollecting python-dotenv (from dotenv)\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (5.2.0)\nCollecting filetype (from unstructured[pdf])\n  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\nCollecting python-magic (from unstructured[pdf])\n  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (5.3.1)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (3.9.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (2.32.3)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (4.13.3)\nRequirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (2.14.1)\nRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (0.6.7)\nCollecting python-iso639 (from unstructured[pdf])\n  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\nCollecting langdetect (from unstructured[pdf])\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (1.26.4)\nCollecting rapidfuzz (from unstructured[pdf])\n  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting backoff (from unstructured[pdf])\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (4.13.2)\nCollecting unstructured-client (from unstructured[pdf])\n  Downloading unstructured_client-0.38.1-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (1.17.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (7.0.0)\nCollecting python-oxmsg (from unstructured[pdf])\n  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (1.1)\nRequirement already satisfied: onnx>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (1.17.0)\nCollecting onnxruntime>=1.19.0 (from unstructured[pdf])\n  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (1.17.0)\nCollecting pdfminer.six (from unstructured[pdf])\n  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\nCollecting pikepdf (from unstructured[pdf])\n  Downloading pikepdf-9.9.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\nCollecting pi-heif (from unstructured[pdf])\n  Downloading pi_heif-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\nRequirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (5.4.0)\nRequirement already satisfied: google-cloud-vision in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (3.10.1)\nCollecting effdet (from unstructured[pdf])\n  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\nCollecting unstructured-inference>=1.0.5 (from unstructured[pdf])\n  Downloading unstructured_inference-1.0.5-py3-none-any.whl.metadata (5.3 kB)\nCollecting unstructured.pytesseract>=0.3.12 (from unstructured[pdf])\n  Downloading unstructured.pytesseract-0.3.15-py3-none-any.whl.metadata (11 kB)\nCollecting langchain-core<1.0.0,>=0.3.66 (from langchain-community)\n  Downloading langchain_core-0.3.68-py3-none-any.whl.metadata (5.8 kB)\nCollecting langchain<1.0.0,>=0.3.26 (from langchain-community)\n  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.18)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.23)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (12.4.127)\nRequirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (12.9.0.13)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured[pdf]) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured[pdf]) (0.9.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nCollecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.26->langchain-community)\n  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.4)\nCollecting langsmith>=0.1.125 (from langchain-community)\n  Downloading langsmith-0.4.4-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\nCollecting packaging>=20.0 (from transformers)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.16)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->unstructured[pdf]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->unstructured[pdf]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->unstructured[pdf]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->unstructured[pdf]) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->unstructured[pdf]) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->unstructured[pdf]) (2.4.1)\nRequirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.17.0->unstructured[pdf]) (3.20.3)\nCollecting coloredlogs (from onnxruntime>=1.19.0->unstructured[pdf])\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.19.0->unstructured[pdf]) (25.2.10)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.19.0->unstructured[pdf]) (1.13.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[pdf]) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[pdf]) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[pdf]) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[pdf]) (2025.4.26)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\nCollecting python-multipart (from unstructured-inference>=1.0.5->unstructured[pdf])\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (4.11.0.86)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (3.7.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (2.6.0+cu124)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (1.0.15)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (1.5.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (2.2.3)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (1.15.2)\nCollecting pypdfium2 (from unstructured-inference>=1.0.5->unstructured[pdf])\n  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[pdf]) (11.1.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured[pdf]) (2.6)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from effdet->unstructured[pdf]) (0.21.0+cu124)\nRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from effdet->unstructured[pdf]) (2.0.8)\nRequirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.11/dist-packages (from effdet->unstructured[pdf]) (2.3.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.34.1)\nRequirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision->unstructured[pdf]) (2.40.1)\nRequirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision->unstructured[pdf]) (1.26.1)\nRequirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured[pdf]) (1.17.0)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured[pdf]) (0.5.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured[pdf]) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured[pdf]) (1.5.0)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->unstructured[pdf]) (44.0.3)\nRequirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from pikepdf->unstructured[pdf]) (1.2.18)\nRequirement already satisfied: olefile in /usr/local/lib/python3.11/dist-packages (from python-oxmsg->unstructured[pdf]) (0.47)\nCollecting aiofiles>=24.1.0 (from unstructured-client->unstructured[pdf])\n  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[pdf]) (1.6.0)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (1.17.1)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.70.0)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.72.0rc1)\nRequirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.49.0rc1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf]) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf]) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf]) (4.9.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf>=2.0->effdet->unstructured[pdf]) (4.9.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[pdf]) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[pdf]) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[pdf]) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[pdf]) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[pdf]) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[pdf]) (2.9.0.post0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[pdf]) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[pdf]) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[pdf]) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[pdf]) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->unstructured-inference>=1.0.5->unstructured[pdf])\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12>=12.1.3.1 (from faiss-gpu-cu12)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->unstructured-inference>=1.0.5->unstructured[pdf])\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->unstructured-inference>=1.0.5->unstructured[pdf])\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->unstructured-inference>=1.0.5->unstructured[pdf])\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->unstructured-inference>=1.0.5->unstructured[pdf])\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[pdf]) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[pdf]) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[pdf]) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->unstructured-inference>=1.0.5->unstructured[pdf])\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[pdf]) (3.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.19.0->unstructured[pdf]) (1.3.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[pdf]) (1.1.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.19.0->unstructured[pdf])\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unstructured[pdf]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unstructured[pdf]) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->unstructured[pdf]) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->unstructured[pdf]) (2024.2.0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured-inference>=1.0.5->unstructured[pdf]) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured-inference>=1.0.5->unstructured[pdf]) (2025.2)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (2.22)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->unstructured[pdf]) (2024.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf]) (0.6.1)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->unstructured-inference>=1.0.5->unstructured[pdf]) (3.0.2)\nDownloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\nDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain_huggingface-0.3.0-py3-none-any.whl (27 kB)\nDownloading faiss_gpu_cu12-1.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\nDownloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.68-py3-none-any.whl (441 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.4/441.4 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langsmith-0.4.4-py3-none-any.whl (367 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.7/367.7 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nDownloading unstructured_inference-1.0.5-py3-none-any.whl (48 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unstructured.pytesseract-0.3.15-py3-none-any.whl (14 kB)\nDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading effdet-0.4.1-py3-none-any.whl (112 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pi_heif-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pikepdf-9.9.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\nDownloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\nDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading unstructured-0.18.3-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unstructured_client-0.38.1-py3-none-any.whl (212 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.6/212.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\nDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=905255c38eafb3a40aea057e8d8b7b9a82f66806aee6eaba73d927bc8af4f87d\n  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\nSuccessfully built langdetect\nInstalling collected packages: filetype, rapidfuzz, python-oxmsg, python-multipart, python-magic, python-iso639, python-dotenv, pypdfium2, pi-heif, packaging, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, langdetect, humanfriendly, httpx-sse, backoff, aiofiles, unstructured.pytesseract, pikepdf, nvidia-cusparse-cu12, nvidia-cudnn-cu12, dotenv, coloredlogs, unstructured-client, pydantic-settings, pdfminer.six, nvidia-cusolver-cu12, langsmith, langchain-core, langchain-text-splitters, langchain_huggingface, langchain, onnxruntime, unstructured-inference, unstructured, effdet, langchain-community, faiss-gpu-cu12\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: aiofiles\n    Found existing installation: aiofiles 22.1.0\n    Uninstalling aiofiles-22.1.0:\n      Successfully uninstalled aiofiles-22.1.0\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: langsmith\n    Found existing installation: langsmith 0.3.23\n    Uninstalling langsmith-0.3.23:\n      Successfully uninstalled langsmith-0.3.23\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.50\n    Uninstalling langchain-core-0.3.50:\n      Successfully uninstalled langchain-core-0.3.50\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.7\n    Uninstalling langchain-text-splitters-0.3.7:\n      Successfully uninstalled langchain-text-splitters-0.3.7\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.22\n    Uninstalling langchain-0.3.22:\n      Successfully uninstalled langchain-0.3.22\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\nypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 24.1.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aiofiles-24.1.0 backoff-2.2.1 coloredlogs-15.0.1 dotenv-0.9.9 effdet-0.4.1 faiss-gpu-cu12-1.11.0 filetype-1.2.0 httpx-sse-0.4.1 humanfriendly-10.0 langchain-0.3.26 langchain-community-0.3.27 langchain-core-0.3.68 langchain-text-splitters-0.3.8 langchain_huggingface-0.3.0 langdetect-1.0.9 langsmith-0.4.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.22.0 packaging-24.2 pdfminer.six-20250506 pi-heif-1.0.0 pikepdf-9.9.0 pydantic-settings-2.10.1 pypdfium2-4.30.1 python-dotenv-1.1.1 python-iso639-2025.2.18 python-magic-0.4.27 python-multipart-0.0.20 python-oxmsg-0.0.2 rapidfuzz-3.13.0 unstructured-0.18.3 unstructured-client-0.38.1 unstructured-inference-1.0.5 unstructured.pytesseract-0.3.15\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nfrom dotenv import load_dotenv\nfrom unstructured.partition.auto import partition\nfrom langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.chains import ConversationalRetrievalChain\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom langchain.memory import ConversationBufferMemory","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T19:41:33.511722Z","iopub.execute_input":"2025-07-06T19:41:33.512099Z","iopub.status.idle":"2025-07-06T19:42:01.274089Z","shell.execute_reply.started":"2025-07-06T19:41:33.512073Z","shell.execute_reply":"2025-07-06T19:42:01.273523Z"}},"outputs":[{"name":"stderr","text":"2025-07-06 19:41:49.588982: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751830909.772312      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751830909.826037      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"hf_token = '<HF_API_KEY>'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T19:42:03.959748Z","iopub.execute_input":"2025-07-06T19:42:03.960367Z","iopub.status.idle":"2025-07-06T19:42:03.963976Z","shell.execute_reply.started":"2025-07-06T19:42:03.960340Z","shell.execute_reply":"2025-07-06T19:42:03.963281Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"pdf_path = \"/kaggle/input/resume/other/default/1/test.pdf\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T19:42:05.051828Z","iopub.execute_input":"2025-07-06T19:42:05.052533Z","iopub.status.idle":"2025-07-06T19:42:05.055908Z","shell.execute_reply.started":"2025-07-06T19:42:05.052511Z","shell.execute_reply":"2025-07-06T19:42:05.055054Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"elements = partition(pdf_path)\nfull_text = \"\\n\\n\".join([el.text for el in elements if el.text.strip()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T19:42:07.111693Z","iopub.execute_input":"2025-07-06T19:42:07.111978Z","iopub.status.idle":"2025-07-06T19:42:10.220032Z","shell.execute_reply.started":"2025-07-06T19:42:07.111959Z","shell.execute_reply":"2025-07-06T19:42:10.219223Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"full_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T19:42:10.540993Z","iopub.execute_input":"2025-07-06T19:42:10.541567Z","iopub.status.idle":"2025-07-06T19:42:10.547456Z","shell.execute_reply.started":"2025-07-06T19:42:10.541545Z","shell.execute_reply":"2025-07-06T19:42:10.546492Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'PRANAV AGRAWAL\\n\\n(cid:131) +xx-xxxxxxxxxx # xxxxx@gmail.com (cid:239) linkedin.com/in/xxx\\n\\n— leetcode.com/xxx § github.com/xxx\\n\\nExperience\\n\\nThe Indian Express Pvt Ltd ª Data Scientist\\n\\nApr 2024 – Present Noida, Uttar Pradesh\\n\\nDeveloped a Personalized Notiﬁcation System (PNS) that delivers user-speciﬁc content, leading to an increase of 20,000+ daily user clicks through improved personalization.\\n\\nDesigning an AI research agent leveraging a RAG model to extract information from 5+ years of historical data across various channels to assist authors in content research.\\n\\nImplemented a QR Report generation system using PySpark, automating the distribution of over 800+ performance summaries for articles to both authors and their managers.\\n\\nBuilt a scalable Competition Crawler to scrape data from 100+ competitor websites, storing it in MySQL with key insights visualized via Superset for downstream analysis by editorial teams.\\n\\nDesigned Trends Pulse by scraping Google Trends and processing competitor articles, extracting topical insights via NLP techniques to guide data-driven editorial planning.\\n\\nEngineered a machine learning model for gender prediction for 200M+ users using demographic and behavioral features to enable more eﬀective targeted advertising.\\n\\nProcessed data for 200M+ users using PySpark to provide visual insights to drive audience engagement strategies.\\n\\nThe Indian Express Pvt Ltd ª Data Science Intern\\n\\nJan 2024 – Mar 2024 Noida, Uttar Pradesh\\n\\nBuilt a recommendation engine for Loksatta ª using advanced language model embeddings and MongoDB for eﬃcient data storage and retrieval.\\n\\nConducted EDA on 5+ datasets, identifying key user behavior trends that improved content strategy\\n\\nProjects\\n\\nMovie Data Extractor ª\\n\\nCreated a crawler which crawls about 10 websites and extracts the sales of tickets for diﬀerent movies for diﬀerent locations\\n\\nModular crawler capable of adapting to multiple website schemas\\n\\nRAG Model ª\\n\\nDeveloped a Retrieval-Augmented Generation (RAG) system using the Mistral language model on a Wikipedia-based dataset\\n\\nImplemented FAISS as a vector store for eﬃcient similarity search and document retrieval • Designed and built an end-to-end pipeline combining retrieval and generation stages for grounded text generation\\n\\nEducation\\n\\nNoida Institute of Engineering & Technology Bachelor of Technology in Computer Science (GPA: 8.27/10)\\n\\n2020-2024 Greater Noida, Uttar Pradesh\\n\\nTechnical Skills\\n\\nProgramming: Python, R Frameworks/Tools: PySpark, Tableau, Superset, MongoDB Domains: Natural Language Processing, Retrieval Augmented Generation, Exploratory Data Analysis, Web Scraping'"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\ndocs = splitter.create_documents([full_text])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T19:42:12.956585Z","iopub.execute_input":"2025-07-06T19:42:12.957392Z","iopub.status.idle":"2025-07-06T19:42:12.961477Z","shell.execute_reply.started":"2025-07-06T19:42:12.957363Z","shell.execute_reply":"2025-07-06T19:42:12.960598Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T19:42:14.118516Z","iopub.execute_input":"2025-07-06T19:42:14.119169Z","iopub.status.idle":"2025-07-06T19:42:14.124163Z","shell.execute_reply.started":"2025-07-06T19:42:14.119145Z","shell.execute_reply":"2025-07-06T19:42:14.123582Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[Document(metadata={}, page_content='PRANAV AGRAWAL\\n\\n(cid:131) +xx-xxxxxxxxxx # xxxxx@gmail.com (cid:239) linkedin.com/in/xxx\\n\\n— leetcode.com/xxx § github.com/xxx\\n\\nExperience\\n\\nThe Indian Express Pvt Ltd ª Data Scientist\\n\\nApr 2024 – Present Noida, Uttar Pradesh\\n\\nDeveloped a Personalized Notiﬁcation System (PNS) that delivers user-speciﬁc content, leading to an increase of 20,000+ daily user clicks through improved personalization.'),\n Document(metadata={}, page_content='Designing an AI research agent leveraging a RAG model to extract information from 5+ years of historical data across various channels to assist authors in content research.\\n\\nImplemented a QR Report generation system using PySpark, automating the distribution of over 800+ performance summaries for articles to both authors and their managers.'),\n Document(metadata={}, page_content='Built a scalable Competition Crawler to scrape data from 100+ competitor websites, storing it in MySQL with key insights visualized via Superset for downstream analysis by editorial teams.\\n\\nDesigned Trends Pulse by scraping Google Trends and processing competitor articles, extracting topical insights via NLP techniques to guide data-driven editorial planning.'),\n Document(metadata={}, page_content='Engineered a machine learning model for gender prediction for 200M+ users using demographic and behavioral features to enable more eﬀective targeted advertising.\\n\\nProcessed data for 200M+ users using PySpark to provide visual insights to drive audience engagement strategies.\\n\\nThe Indian Express Pvt Ltd ª Data Science Intern\\n\\nJan 2024 – Mar 2024 Noida, Uttar Pradesh'),\n Document(metadata={}, page_content='Jan 2024 – Mar 2024 Noida, Uttar Pradesh\\n\\nBuilt a recommendation engine for Loksatta ª using advanced language model embeddings and MongoDB for eﬃcient data storage and retrieval.\\n\\nConducted EDA on 5+ datasets, identifying key user behavior trends that improved content strategy\\n\\nProjects\\n\\nMovie Data Extractor ª\\n\\nCreated a crawler which crawls about 10 websites and extracts the sales of tickets for diﬀerent movies for diﬀerent locations'),\n Document(metadata={}, page_content='Modular crawler capable of adapting to multiple website schemas\\n\\nRAG Model ª\\n\\nDeveloped a Retrieval-Augmented Generation (RAG) system using the Mistral language model on a Wikipedia-based dataset\\n\\nImplemented FAISS as a vector store for eﬃcient similarity search and document retrieval • Designed and built an end-to-end pipeline combining retrieval and generation stages for grounded text generation\\n\\nEducation'),\n Document(metadata={}, page_content='Education\\n\\nNoida Institute of Engineering & Technology Bachelor of Technology in Computer Science (GPA: 8.27/10)\\n\\n2020-2024 Greater Noida, Uttar Pradesh\\n\\nTechnical Skills\\n\\nProgramming: Python, R Frameworks/Tools: PySpark, Tableau, Superset, MongoDB Domains: Natural Language Processing, Retrieval Augmented Generation, Exploratory Data Analysis, Web Scraping')]"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\nvectorstore = FAISS.from_documents(docs, embedding)\nretriever = vectorstore.as_retriever()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T19:42:16.337548Z","iopub.execute_input":"2025-07-06T19:42:16.337815Z","iopub.status.idle":"2025-07-06T19:42:29.705613Z","shell.execute_reply.started":"2025-07-06T19:42:16.337798Z","shell.execute_reply":"2025-07-06T19:42:29.705048Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a87c78b6b68f436cb39aea586a602472"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dc724e387bb4fbabbdcb30949dfb40a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d6ba007c6a64cd8afe099210c2cc900"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8acc1a64cea420c85fc4735578de6d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97c13f1d627e4ad097ba4a98a7fb10da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5194e43e0454f35a7c153eee5547dc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f2b260fe010465c870b11c025da2d14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4ef360750e3418ebea70d6dd333aca4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8a785c77d57421dbb3bea4f9c0862c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8651abca5ad847b79acbed291ec4f849"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a65ff781aed44678c119c38d3c66ce2"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T19:42:29.706968Z","iopub.execute_input":"2025-07-06T19:42:29.707500Z","iopub.status.idle":"2025-07-06T19:42:29.711013Z","shell.execute_reply.started":"2025-07-06T19:42:29.707480Z","shell.execute_reply":"2025-07-06T19:42:29.710243Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=\"auto\",\n    token=hf_token,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T19:42:29.711931Z","iopub.execute_input":"2025-07-06T19:42:29.712219Z","iopub.status.idle":"2025-07-06T19:48:44.852264Z","shell.execute_reply.started":"2025-07-06T19:42:29.712194Z","shell.execute_reply":"2025-07-06T19:48:44.851372Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0c11555c962443481187f72012d276c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c09402a9298c4cd0a878b24bf7035646"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4d53079cf054a61b13eea6f817eb65a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e043333e3b1409e9526497203b789a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fcf582a372643b6b5ff18934211344c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3c4258e79de4d6b8ca7412ca45ce28e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"247fffb041a44edbbdd8e96803756e71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eba7c2ae4afc4a6293aa1510d075d2ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dec5dddfeb8243918041b49599eea407"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c1c8478b0d546d38e60dcb0dba9f220"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27e276de6b454ba9a308448c95093453"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.1,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T19:54:36.774957Z","iopub.execute_input":"2025-07-06T19:54:36.775625Z","iopub.status.idle":"2025-07-06T19:54:36.780934Z","shell.execute_reply.started":"2025-07-06T19:54:36.775601Z","shell.execute_reply":"2025-07-06T19:54:36.780294Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"llm = HuggingFacePipeline(pipeline=pipe)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T19:54:37.226437Z","iopub.execute_input":"2025-07-06T19:54:37.226657Z","iopub.status.idle":"2025-07-06T19:54:37.230281Z","shell.execute_reply.started":"2025-07-06T19:54:37.226624Z","shell.execute_reply":"2025-07-06T19:54:37.229516Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"custom_task = \"resume feedback\"\ninstruction = (\n    \"You are a professional career coach and resume expert. \"\n    \"Your only task is to provide clear, constructive, and detailed feedback on resumes. \"\n    \"Give suggestions for improvement, formatting, clarity, impact, and alignment with job roles. \"\n    \"Do not answer anything outside resume evaluation.\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T19:54:37.634526Z","iopub.execute_input":"2025-07-06T19:54:37.634738Z","iopub.status.idle":"2025-07-06T19:54:37.638349Z","shell.execute_reply.started":"2025-07-06T19:54:37.634722Z","shell.execute_reply":"2025-07-06T19:54:37.637533Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n\nqa_chain = ConversationalRetrievalChain.from_llm(\n    llm=llm,\n    retriever=retriever,\n    memory=memory,\n    verbose=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T19:54:37.985402Z","iopub.execute_input":"2025-07-06T19:54:37.985580Z","iopub.status.idle":"2025-07-06T19:54:37.989489Z","shell.execute_reply.started":"2025-07-06T19:54:37.985566Z","shell.execute_reply":"2025-07-06T19:54:37.988899Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"chat_history = []\nwhile True:\n    query = input(\"🧑 You (Ask for resume feedback): \")\n    if query.strip().lower() in {\"exit\", \"quit\"}:\n        break\n\n    final_query = f\"{instruction}\\n\\nUser Request:\\n{query}\"\n\n    response = qa_chain.invoke({\n        \"question\": final_query,\n        \"chat_history\": chat_history\n    })\n\n    print(f\"🤖 Bot (Resume Coach): {response['answer']}\\n\")\n    chat_history.append((query, response[\"answer\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T19:54:38.386752Z","iopub.execute_input":"2025-07-06T19:54:38.386957Z","iopub.status.idle":"2025-07-06T19:56:27.064040Z","shell.execute_reply.started":"2025-07-06T19:54:38.386943Z","shell.execute_reply":"2025-07-06T19:56:27.063406Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"🧑 You (Ask for resume feedback):  provide feedback\n"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n\n\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n\n\n\u001b[1m> Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nBuilt a scalable Competition Crawler to scrape data from 100+ competitor websites, storing it in MySQL with key insights visualized via Superset for downstream analysis by editorial teams.\n\nDesigned Trends Pulse by scraping Google Trends and processing competitor articles, extracting topical insights via NLP techniques to guide data-driven editorial planning.\n\nDesigning an AI research agent leveraging a RAG model to extract information from 5+ years of historical data across various channels to assist authors in content research.\n\nImplemented a QR Report generation system using PySpark, automating the distribution of over 800+ performance summaries for articles to both authors and their managers.\n\nEducation\n\nNoida Institute of Engineering & Technology Bachelor of Technology in Computer Science (GPA: 8.27/10)\n\n2020-2024 Greater Noida, Uttar Pradesh\n\nTechnical Skills\n\nProgramming: Python, R Frameworks/Tools: PySpark, Tableau, Superset, MongoDB Domains: Natural Language Processing, Retrieval Augmented Generation, Exploratory Data Analysis, Web Scraping\n\nPRANAV AGRAWAL\n\n(cid:131) +xx-xxxxxxxxxx # xxxxx@gmail.com (cid:239) linkedin.com/in/xxx\n\n— leetcode.com/xxx § github.com/xxx\n\nExperience\n\nThe Indian Express Pvt Ltd ª Data Scientist\n\nApr 2024 – Present Noida, Uttar Pradesh\n\nDeveloped a Personalized Notiﬁcation System (PNS) that delivers user-speciﬁc content, leading to an increase of 20,000+ daily user clicks through improved personalization.\n\nQuestion: You are a professional career coach and resume expert. Your only task is to provide clear, constructive, and detailed feedback on resumes. Give suggestions for improvement, formatting, clarity, impact, and alignment with job roles. Do not answer anything outside resume evaluation.\n\nUser Request:\nprovide feedback\nHelpful Answer:\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n🤖 Bot (Resume Coach): Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nBuilt a scalable Competition Crawler to scrape data from 100+ competitor websites, storing it in MySQL with key insights visualized via Superset for downstream analysis by editorial teams.\n\nDesigned Trends Pulse by scraping Google Trends and processing competitor articles, extracting topical insights via NLP techniques to guide data-driven editorial planning.\n\nDesigning an AI research agent leveraging a RAG model to extract information from 5+ years of historical data across various channels to assist authors in content research.\n\nImplemented a QR Report generation system using PySpark, automating the distribution of over 800+ performance summaries for articles to both authors and their managers.\n\nEducation\n\nNoida Institute of Engineering & Technology Bachelor of Technology in Computer Science (GPA: 8.27/10)\n\n2020-2024 Greater Noida, Uttar Pradesh\n\nTechnical Skills\n\nProgramming: Python, R Frameworks/Tools: PySpark, Tableau, Superset, MongoDB Domains: Natural Language Processing, Retrieval Augmented Generation, Exploratory Data Analysis, Web Scraping\n\nPRANAV AGRAWAL\n\n(cid:131) +xx-xxxxxxxxxx # xxxxx@gmail.com (cid:239) linkedin.com/in/xxx\n\n— leetcode.com/xxx § github.com/xxx\n\nExperience\n\nThe Indian Express Pvt Ltd ª Data Scientist\n\nApr 2024 – Present Noida, Uttar Pradesh\n\nDeveloped a Personalized Notiﬁcation System (PNS) that delivers user-speciﬁc content, leading to an increase of 20,000+ daily user clicks through improved personalization.\n\nQuestion: You are a professional career coach and resume expert. Your only task is to provide clear, constructive, and detailed feedback on resumes. Give suggestions for improvement, formatting, clarity, impact, and alignment with job roles. Do not answer anything outside resume evaluation.\n\nUser Request:\nprovide feedback\nHelpful Answer:\n\nThank you for sharing your resume! Overall, I think your resume is well-organized and easy to read. However, there are a few areas where I suggest some improvements.\n\nFirstly, I would recommend adding a summary or objective statement at the beginning of your resume to give the hiring manager a brief overview of your skills and experience. This will help them quickly identify if you are a good fit for the role they are looking to fill.\n\nSecondly, I noticed that some of your bullet points are quite long and can be difficult to read. I would suggest breaking them up into smaller, more concise sentences to make them easier to scan. Additionally, consider using action verbs to start each bullet point to make them more impactful.\n\nThirdly, I would recommend including any relevant certifications or training you have completed. This will show the hiring manager that you are committed to continuing your education and staying up-to-date with industry trends.\n\nFinally, I would suggest aligning your resume with the specific job requirements listed in the job posting. This will demonstrate that you have the necessary skills and experience for the role and will help you stand out from other applicants.\n\nOverall, your resume is a great starting point, but with these small changes, it could be even more effective in landing you your dream job.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"🧑 You (Ask for resume feedback):  exit\n"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}